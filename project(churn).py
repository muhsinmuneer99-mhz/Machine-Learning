# -*- coding: utf-8 -*-
"""Project(churn).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PQ4fbZVjvonaO7h8IekuxvNpx4JpApWs

In this project we will be building a model that Predicts customer churn with Machine Learning.

Prediction of Customer Churn means our beloved customers with the intention of leaving us in the future.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report,accuracy_score,ConfusionMatrixDisplay
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from imblearn.over_sampling import SMOTE

df=pd.read_csv('/content/churn.csv')
df.head()

df.shape

df.columns

df.info()

"""**STATISTICAL SUMMARY OF DATASET**"""

df.describe()

"""**CHECKING WHETHER THERE IS ANY MISSING VALUES IN THE DATASET**"""

df.isna().sum()

# to get churn count
df['Churn'].value_counts()

"""**CHECK DATASET IS BALANCED OR NOT**"""

# visualize the churn count of customer
sns.countplot(df['Churn'])

"""**Percentage of customers Stay and Left the Company**"""

stay=df[df.Churn == 'No'].shape[0]  # shape of [0] used to print only rows
left=df[df.Churn == 'Yes'].shape[0]

print(stay/(stay+left)*100,'% of customer stayed')
print(left/(stay+left)*100,'% of customer left')

plt.pie(df['Churn'].value_counts(),labels=df['Churn'].unique(),autopct='%0.2f')

df_cat=[x for x in df.columns if df[x].dtype=='O']
df_cat

# visualize the churn count for both males and females
sns.countplot(x='gender',hue='Churn',data=df)

"""**Visualize the Churn Count  For Internet Service**"""

sns.countplot(x='InternetService',hue='Churn',data=df)

"""**Numerical Columns**"""

df_num=[x for x in df.columns if(df[x].dtype!='O') & (x not in ['Churn'])]
df_num

"""**distribution of Numerical Features**"""

plt.figure(figsize=(20,60),facecolor='white')
plotnumber=1
for i in df_num:
  plt.subplot(12,3,plotnumber)
  sns.distplot(df[i])
  plt.title('Countplot of {}'.format(i))
  plt.tight_layout()
  plotnumber+=1
plt.show()

# to visualize numeric data
numericfeatures=['tenure','MonthlyCharges']
fig, ax = plt.subplots(1,2, figsize=(28, 8))
df[df.Churn=='No'][numericfeatures].hist(bins=20,color='blue',alpha=0.5,ax=ax)
df[df.Churn=='Yes'][numericfeatures].hist(bins=20,color='orange',alpha=0.5,ax=ax)

# convert all non-numeric into numeric
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()

for x in df.columns:
  if df[x].dtype==np.number:
    continue
  df[x]=le.fit_transform(df[x])

df.head()   # here all columns are in numeric data

df.info()  # so all in integer and float

# x and y seperation
x=df.iloc[:,:-1].values
y=df.iloc[:,-1].values

# scale the data
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
x=scaler.fit_transform(x)
x

"""**PCA**"""

pca=PCA()
pca.fit(x)

pca.explained_variance_ratio_

"""**Scree Plot Criterion**"""

fig,ax=plt.subplots()
xi=np.arange(1,x.shape[1]+1,1)
yi=np.cumsum(pca.explained_variance_ratio_)
plt.plot(xi,yi,marker='o',linestyle='--',color='b')
plt.xlabel('Number of Components')
plt.xticks(xi)
plt.ylabel('Cumulative variance (%)')
plt.title('Scree Plot')
ax.set_ylim([0.70,1.01])
plt.axhline(y=0.95,color='r',linestyle='--')
ax.grid('both')
plt.xticks(rotation=90)
plt.show()

pca=PCA(n_components=0.95,random_state=42)
pca.fit(x)
x_pca=pca.transform(x)
print('Shape of x before PCA:',x.shape)
print('Shape of x after PCA:',x_pca.shape)

# train test split
xtrain,xtest,ytrain,ytest=train_test_split(x_pca,y,test_size=0.30,random_state=42)
xtrain
xtest
ytrain
ytest

"""**Creating Different Models using Imbalanced Dataset**

**KNN**
"""

model=KNeighborsClassifier(n_neighbors=5)
model.fit(xtrain,ytrain)

ypredknn=model.predict(xtest)

print(classification_report(ytest,ypredknn))
print(ConfusionMatrixDisplay.from_predictions(ytest,ypredknn))

"""**Logistic Regressioin**"""

lr=LogisticRegression()
lr.fit(xtrain,ytrain)

ypredlr=lr.predict(xtest)

print(classification_report(ypredlr,ytest))
print(ConfusionMatrixDisplay.from_predictions(ytest,ypredlr))

"""**Naive Bayes**"""

nb=GaussianNB()
nb.fit(xtrain,ytrain)

yprednb=nb.predict(xtest)
yprednb

print(classification_report(yprednb,ytest))
print(ConfusionMatrixDisplay.from_predictions(ytest,yprednb))

"""**SVM**"""

svc=SVC()
svc.fit(xtrain,ytrain)

ypredsvc=svc.predict(xtest)

print(classification_report(ypredsvc,ytest))
print(ConfusionMatrixDisplay.from_predictions(ytest,ypredsvc))

"""**Decision Tree(gini)**"""

tree=DecisionTreeClassifier()
tree.fit(xtrain,ytrain)

ypredtree=tree.predict(xtest)

print(classification_report(ypredtree,ytest))
print(ConfusionMatrixDisplay.from_predictions(ytest,ypredtree))

"""**Decision Tree(entropy)**"""

tretp=DecisionTreeClassifier(criterion='entropy')
tretp.fit(xtrain,ytrain)

ypredtree2=tretp.predict(xtest)

print(classification_report(ypredtree2,ytest))
print(ConfusionMatrixDisplay.from_predictions(ytest,ypredtree2))

"""**Random forest**"""

from sklearn.model_selection import RandomizedSearchCV

#RandomizedSearchCV,
# random forest
rf=RandomForestClassifier()
rf_p_dist={'max_depth':[3,5,10,None],'n_estimators':[10,100,200,300,400,500]}
rdmsearch=RandomizedSearchCV(rf,param_distributions=rf_p_dist,n_iter=10,cv=5)
rdmsearch.fit(xtrain,ytrain)

#ypred=model.predict(xtest)
#ypred

rdmsearch.best_params_

rfcl=RandomForestClassifier(max_depth=10,n_estimators=400)
rfcl.fit(xtrain,ytrain)

ypredrf=rfcl.predict(xtest)

print(classification_report(ypredrf,ytest))
print(ConfusionMatrixDisplay.from_predictions(ytest,ypredrf))

"""**Different Algorithms On Balanced Dataset(smote)**"""

# using oversampling technique smote
oversample=SMOTE()
xo,yo=oversample.fit_resample(x_pca,y)

xtrain_new,xtest_new,ytrain_new,ytest_new=train_test_split(xo,yo,test_size=0.20,random_state=42)

"""**KNN**"""

knn=KNeighborsClassifier(n_neighbors=5)
knn.fit(xtrain_new,ytrain_new)
y_predknnb=knn.predict(xtest_new)

print(classification_report(y_predknnb,ytest_new))
print(ConfusionMatrixDisplay.from_predictions(ytest_new,y_predknnb))

"""**Logistic Regression**"""

lr=LogisticRegression()
lr.fit(xtrain_new,ytrain_new)

ypred_lr=lr.predict(xtest_new)
ypred_lr

print(classification_report(ypred_lr,ytest_new))
print(ConfusionMatrixDisplay.from_predictions(ytest_new,ypred_lr))

"""**Naive Bayes**"""

nb=GaussianNB()
nb.fit(xtrain_new,ytrain_new)

ypred_nb=nb.predict(xtest_new)
ypred_nb

print(classification_report(ypred_nb,ytest_new))
print(ConfusionMatrixDisplay.from_predictions(ytest_new,ypred_nb))

"""**SVM**"""

#svc
svc=SVC()
svc.fit(xtrain_new,ytrain_new)

ypred_svc=svc.predict(xtest_new)
ypred_svc

print(classification_report(ypred_svc,ytest_new))
print(ConfusionMatrixDisplay.from_predictions(ytest_new,ypred_svc))

"""**Decision Tree(gini)**"""

# dtree
tree=DecisionTreeClassifier()
tree.fit(xtrain_new,ytrain_new)

ypred_tree=tree.predict(xtest_new)

print(classification_report(ypred_tree,ytest_new))
print(ConfusionMatrixDisplay.from_predictions(ytest_new,ypred_tree))

"""**Decision Tree(entropy)**"""

tretp=DecisionTreeClassifier(criterion='entropy')
tretp.fit(xtrain_new,ytrain_new)

ypred_tree2=tretp.predict(xtest_new)

print(classification_report(ypred_tree2,ytest_new))
print(ConfusionMatrixDisplay.from_predictions(ytest_new,ypred_tree2))

"""**Random Forest**"""

#rf
rfcl=RandomForestClassifier(max_depth=10,n_estimators=400)
rfcl.fit(xtrain_new,ytrain_new)

ypred_rf=rfcl.predict(xtest_new)

print(classification_report(ypred_rf,ytest_new))
print(ConfusionMatrixDisplay.from_predictions(ytest_new,ypred_rf))

accuracy_df=pd.DataFrame()

accuracy_df['Classifier']=['KNN','Logistic Regression','Naive Bayes','SVM','Decision Tree(gini)','Decision Tree(entropy)','Random Forest']
accuracy_df['Accuracy of Imbalanced Data']=[accuracy_score(ypredknn,ytest),accuracy_score(ypredlr,ytest),accuracy_score(yprednb,ytest),
                                            accuracy_score(ypredsvc,ytest),accuracy_score(ypredtree,ytest),accuracy_score(ypredtree2,ytest),
                                            accuracy_score(ypredrf,ytest)]
accuracy_df['Accuracy of Balanced Data']=[accuracy_score(y_predknnb,ytest_new),accuracy_score(ypred_lr,ytest_new),accuracy_score(ypred_nb,ytest_new),
                                          accuracy_score(ypred_svc,ytest_new),accuracy_score(ypred_tree,ytest_new),accuracy_score(ypred_tree2,ytest_new),
                                          accuracy_score(ypred_rf,ytest_new)]

accuracy_df

"""Model Created Using Random Forest Classifier using Balanced Dataset gives highest accuracy about 83%."""